{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4893ee6",
   "metadata": {},
   "source": [
    "# <center> **Tesina di Fondamenti di Data Science 2022-2023**\n",
    "\n",
    "<br>\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/wallpaper.jpg\" width=\"1300\" height=\"auto\">\n",
    "\n",
    "### <center> _Andrea Spinelli, Raffaele Terracino, Marco Valenti_\n",
    "##### <center> \\<GG/MM/AAAA\\>\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ed5dffb",
   "metadata": {},
   "source": [
    "# __`Indice`__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4687743",
   "metadata": {},
   "source": [
    "###  [`1. - Traccia`](#1-traccia)\n",
    "### [`2. - Pipeline Analysis`](#2-pipeline-analysis)\n",
    "##### &emsp;&emsp; [`2.1 - Data collection`](#21-data-collection)\n",
    "##### &emsp;&emsp; [`2.2 - Data preprocessing`](#22-data-preprocessing)\n",
    "##### &emsp;&emsp; [`2.3 - Analytical processing`](#23-analytical-processing)\n",
    "### [`3. - Risultati`](#3-risultati)\n",
    "##### &emsp;&emsp; [`3.1 - Clustering`](#31-clustering)\n",
    "##### &emsp;&emsp; [`3.2 - Matrix Confusion`](#32-matrix-confusion)\n",
    "### [`4. - Extra`](#)\n",
    "##### &emsp;&emsp; [`4.1 - Subtitle`](#)\n",
    "##### &emsp;&emsp; [`4.2 - Subtitle`](#)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a21597c8",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# __`1 Traccia`__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb5b2ab5",
   "metadata": {},
   "source": [
    "Sia dato il dataset `digits.csv` che contiene le immagini di numeri scritti a mano ($8\\times 8=64$ pixel), inseriti come righe del file, in cui l’ultima colonna di ogni riga rappresenta il target.\n",
    "\n",
    "<br>\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/track1.png\" width=\"800\" height=\"auto\"><br>\n",
    "    <a href=\"https://www.educative.io/answers/what-is-datasetsloaddigits-in-sklearn\"> https://www.educative.io/answers/what-is-datasetsloaddigits-in-sklearn </a>\n",
    "</div>\n",
    "\n",
    "Realizzare un __Progetto di Data Analysis__ che:\n",
    "- Prevede l’`utilizzo della PCA` per proiettare le $64$ dimensioni sulle prime due componenti principali, questi punti sono la proiezione di ciascun punto dati lungo le direzioni con la varianza maggiore;\n",
    "- Prevede la `clusterizzazione dei punti` ($k=10$) tramite tecnica $K$ _- Means_ (basato sul centroide);\n",
    "- Ripete la `clusterizzazione sulle proiezioni sulle prime N componenti principali`, con $N$ che varia tra $2$ e $6$;\n",
    "- Per ogni clusterizzazione produce una `matrice di confusione` che metta a confronto l’accuracy nella classificazione al variare del numero di componenti principali scelte.\n",
    "\n",
    "<br>\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/track2.png\" width=\"800\" height=\"auto\">\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0902fd99",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# __`2 Pipeline Analysis`__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6624065d",
   "metadata": {},
   "source": [
    "\n",
    "### __`2.1 Data collection`__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "958660ca",
   "metadata": {},
   "source": [
    "L'obiettivo del _progetto_ è quello di costruire un modello capace di predire le cifre rappresentate in immagini di $8\\times 8$ pixel, dopodiché effettuare le analisi descritta dalla _Traccia_. \n",
    "Si evidenzia pertanto che se si volessero fare ulteriori analisi di questo tipo, i dataset di training o di test, dovranno essere rappresentati nel modo che andremo a descrivere.\n",
    "\n",
    "Il progetto seguirà la __Pipeline Analysis__ studiata durante il corso, in modo tale da avere maggiore chiarezza durante le varie fasi.<br>\n",
    "Tra le prime fasi della seguente _Pipeline_ v'è innanzitutto la __raccolta dei dati__.\n",
    "\n",
    "Il _dataset_ di riferimento preso a campione per il progetto viene fornito dalla libreria `sklearn` di Python (presente eventualmente nel file `../dataset/digits.csv`), nel quale, dopo un'accurata analisi, il suo tipo di dati si è identificato come __NON interdipendenti__, nello specifico: __Multidimensionali__.\n",
    "\n",
    "Il dataset per l'appunto è conservato in un database contenente delle tuple generiche di cifre (da $0$ a $9$), descritte come una sequenza di $8\\times 8$ pixel con una certa intensità, e alla fine di ciascun tupla è presente la cifra che sta rappresentando. Più formalmente:\n",
    "\n",
    "$D=\\{X_i Y_i : X_i=(x_1,...,x_{64}), \\forall Y_i \\in(0,...,9)\\}$ \n",
    "$\\quad\\quad \\forall x_j \\in(0,...,255)$\n",
    "$\\quad\\quad \\forall i=0,...,n$\n",
    "$\\quad\\quad \\forall j=0,...,64$\n",
    "\n",
    "dove <br>\n",
    "$X_i$ descrive gli $8\\times 8$ pixel di una cifra con una tupla di $64$ attributi numerici con \"intensità\" da $0$ a $255$, <br>\n",
    "$Y_i$ indica la cifra target che sta rappresentando $X_i$.\n",
    "\n",
    "\n",
    "\n",
    ">Nel seguente codice si vanno a importare tutte le librerie necessarie allo scopo del progetto e preleviamo il dataset dalla libreria `sklearn`, mediante il metodo `load_digits()`.<br>\n",
    "Con i metodi `.data` e `.target` preleviamo in corrispondenza tutte le tuple che rappresentano le cifre e le cifre che vengono rappresentate; quest'ultimi vengono assegnati rispettivamente alle variabili `X` e `Y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b02464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import mode\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "digits = load_digits()\n",
    "X, Y = digits.data, digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9c225d",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed748f4b",
   "metadata": {},
   "source": [
    "### __`2.2 Data preprocessing`__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a477323",
   "metadata": {},
   "source": [
    "La fase successiva della nostra Pipeline è la __pre-elaborazione dei dati__, che a sua volta suddividiamo in altre due macro fasi:\n",
    "\n",
    "- __Estrazione delle features__, affinché il modello comprenda che tipo di elaborazione deve intraprendere, è necessario che il dataset abbia delle proprietà che valorizzano i dati al suo interno. La dimensione del nostro problema si circoscrive ai 64 pixel delle immagini, nonché i campi dei record, che utilizzeremo come nostre __Features__. \n",
    "\n",
    "- __Pulizia dei dati__, dato che le considerazioni della fase precedente sono state fatte con il dataset fornito dalla libreria `sklearn`, possiamo assumere che i dati siano puliti; tuttavia, senza voler perdere di generalità, si descrivono comunque le tecniche principali per la pulitura dei dati, per un eventuali analisi, in modo tale che possano essere conformi al modello che si vuole creare:\n",
    "    - __Data Cleaning__, di cui bisogno tenere in considerazione:<br> \n",
    "    La _mancanza di attributi_, la quale, in quanto trattiamo delle immagini, si può risolvere mediante tecniche di Analisi di Immagini, tra cui la media.<br>\n",
    "    L'_identificazione degli outliers_, il valore dei campi dei record devono essere compresi tra $0$ e $255$, pertanto si possono porre delle condizioni in cui il dato assume il valore del limite più vicino; altrimenti si applica la media.\n",
    "    L'_eliminazione dei rumore_, il quale, se non è un problema fisico di un sistema, si può trattare tramite binning.\n",
    "    - __Data Integration__, sappiamo già come i dati devono essere descritti; pertanto, qualsiasi tecnica di _append_ dei dati può essere usata.\n",
    "    - __Data Transformation__, con lo scopo di voler visualizzare i dati in modo tali da poter fare delle osservazioni, i dati possono essere _normalizzati_ mediante la tecnica _Z-Score Normalization_.\n",
    "    - __Data Reduction__, in un caso ipotetico di lavorare con un miliardo di record, si tratterebbe di lavorare con un dataset di minimo poco più 7.5Gb, non eccessivamente oneroso per una macchina prestante, visto però che il modello che stiamo andando a costruire non tratterà una grossa mole di dati è possibile fare delle _aggregazioni_.\n",
    "\n",
    "> La seguente istruzione esegue la Z-Score Normalization per il nostro dataset, riducendo il campo nell'intervallo $(-1,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322fe3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X/1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaa4bcb",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd68fa8",
   "metadata": {},
   "source": [
    "### __`2.3 Analytical processing`__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba56d928",
   "metadata": {},
   "source": [
    "//Some text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab935cc9",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# __`3 Risultati`__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19c195a9",
   "metadata": {},
   "source": [
    "### __`3.1 Clustering`__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8eb5aec9",
   "metadata": {},
   "source": [
    "//Some text\n",
    "\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "977d6e85",
   "metadata": {},
   "source": [
    "### __`3.2 Matrix confusion`__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea9d3123",
   "metadata": {},
   "source": [
    "//Some text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a8e1e15",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# __`4 Extra`__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "804fec58",
   "metadata": {},
   "source": [
    "### __`4.1 Subtitle`__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "525a050e",
   "metadata": {},
   "source": [
    "//Some text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "272dd75b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "598e06ae",
   "metadata": {},
   "source": [
    "### __`4.2 Subtitle`__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80c8108c",
   "metadata": {},
   "source": [
    "//Some text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1011c7ac",
   "metadata": {},
   "source": [
    "//Some text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
